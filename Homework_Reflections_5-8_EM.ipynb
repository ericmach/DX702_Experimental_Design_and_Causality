{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework Reflection 5\n",
    "\n",
    "1. Draw a diagram for the following negative feedback loop:\n",
    "\n",
    "Sweating causes body temperature to decrease.  High body temperature causes sweating.\n",
    "\n",
    "A negative feedback loop means that one thing increases another while the second thing decreases the first.\n",
    "\n",
    "Remember that we are using directed acyclic graphs where two things cannot directly cause each other.\n",
    "\n",
    "2. Describe an example of a positive feedback loop.  This means that one things increases another while the second things also increases the first.\n",
    "\n",
    "3. Draw a diagram for the following situation:\n",
    "\n",
    "Lightning storms frighten away deer and bears, decreasing their population, and cause flowers to grow, increasing their population.\n",
    "Bears eat deer, decreasing their population.\n",
    "Deer eat flowers, decreasing their population.\n",
    "\n",
    "Write a dataset that simulates this situation.  (Show the code.) Include noise / randomness in all cases.\n",
    "\n",
    "Identify a backdoor path with one or more confounders for the relationship between deer and flowers.\n",
    "\n",
    "4. Draw a diagram for a situation of your own invention.  The diagram should include at least four nodes, one confounder, and one collider.  Be sure that it is acyclic (no loops).  Which node would say is most like a treatment (X)?  Which is most like an outcome (Y)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection 5 Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. See attachment\n",
    "\n",
    "2. An example of a positive feedback loop would be population growth. As more people or animals in a population give birth, the population increases, and with a higher population, there are more individuals giving birth, continuing the cycle of population growth.\n",
    "\n",
    "3. See attachment and code below. A possible backdoor path would be lightning's postive effect on flowers and negative effect on deer. This creates a confounding path where deer <-- lightning --> flowers.\n",
    "\n",
    "4. See attachment. In the example, income is the confounder as it causes both the treatment (diet/exercise or X) and the outcome (health or Y). The collider is health as diet and exercise both have an effect on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lightning</th>\n",
       "      <th>Bears</th>\n",
       "      <th>Deer</th>\n",
       "      <th>Flowers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496714</td>\n",
       "      <td>8.177577</td>\n",
       "      <td>7.512492</td>\n",
       "      <td>14.553074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.138264</td>\n",
       "      <td>9.716348</td>\n",
       "      <td>6.542882</td>\n",
       "      <td>16.217061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647689</td>\n",
       "      <td>9.451917</td>\n",
       "      <td>5.236153</td>\n",
       "      <td>20.480357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.523030</td>\n",
       "      <td>7.564311</td>\n",
       "      <td>6.356091</td>\n",
       "      <td>19.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.234153</td>\n",
       "      <td>10.447405</td>\n",
       "      <td>3.429980</td>\n",
       "      <td>22.510447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lightning      Bears      Deer    Flowers\n",
       "0   0.496714   8.177577  7.512492  14.553074\n",
       "1  -0.138264   9.716348  6.542882  16.217061\n",
       "2   0.647689   9.451917  5.236153  20.480357\n",
       "3   1.523030   7.564311  6.356091  19.531600\n",
       "4  -0.234153  10.447405  3.429980  22.510447"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of datapoints\n",
    "n = 100\n",
    "\n",
    "# Simulate lightning  \n",
    "lightning = np.random.normal(loc=0, scale=1, size=n)\n",
    "\n",
    "# Simulate flower population: lightning increases growth\n",
    "flowers = 30 + 2 * lightning + np.random.normal(0, 1, n)\n",
    "\n",
    "# Simulate deer population: lightning decreases growth\n",
    "deer = 25 - 3 * lightning + np.random.normal(0, 1, n)  \n",
    "\n",
    "# Simulate bear population: lightning decreases growth \n",
    "bears = 10 - 2 * lightning + np.random.normal(0, 1, n)  \n",
    "\n",
    "# Adjust deer to account for bear predation\n",
    "deer -= 2 * bears \n",
    "\n",
    "# Adjust flowers to account for deer grazing\n",
    "flowers -= 2 * deer \n",
    "\n",
    "df_sim = pd.DataFrame({\n",
    "    'Lightning': lightning,\n",
    "    'Bears': bears,\n",
    "    'Deer': deer,\n",
    "    'Flowers': flowers\n",
    "})\n",
    "\n",
    "df_sim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework Reflection 6\n",
    "\n",
    "1. What is a potential problem with computing the Marginal Treatment Effect simply by comparing each untreated item to its counterfactual and taking the maximum difference?  (Hint: think of statistics here.  Consider that only the most extreme item ends up being used to estimate the MTE.  That's not necessarily a bad thing; the MTE is supposed to come from the untreated item that will produce the maximum effect.  But there is nevertheless a problem.)\n",
    "Possible answer: We are likely to find the item with the most extreme difference, which may be high simply due to randomness.\n",
    "(Please explain / justify this answer, or give a different one if you can think of one.)\n",
    "\n",
    "2. Propose a solution that remedies this problem and write some code that implements your solution.  It's very important here that you clearly explain what your solution will do.\n",
    "Possible answer: maybe we could take the 90th percentile of the treatment effect and use it as a proxy for the Marginal Treatment Effect.\n",
    "(Either code this answer or choose a different one.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection 6 Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Marginal Treatment Effect (MTE) is meant to reflect the treatment effect for the next unit to be treated, based on who would benefit the most. As noted, this could be approximated by taking the maximum difference by comparing each untreated item to its counterfactual. The problem is that this maximum difference could be due to noisy estimates and randomness, and this is increasingly likely when selecting the maximum difference. Each estimated treatment effect includes noise and so some differences may be large, but due to noise and not the actual treatment effect. This results in identifying an overstated effect when selecting the maximum difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the 90th percentile allows us to still focus on large treatment effects but without the issue of using the maximum difference which is sensitive to random noise. The effects of all large outliers is reduced. In the code below, synthetic data is generated and a regression model is fit to the data with random treatment assignment. The counterfactuals for each instance are computed and predicted and the differences are taken to get the treatment effects. The 90th percentile of these differences is then used as the MTE to avoid the issue of random noise likely influencing the maximum difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Marginal Treatment Effect (90th percentile): 6.44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "num = 1000\n",
    "X = np.random.uniform(0, 1, num)\n",
    "y = np.random.normal(10, 5, num) + X * 5\n",
    "\n",
    "# Randomly assign treatment\n",
    "treated = np.random.binomial(1, 0.5, num)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'X': X,\n",
    "    'y': y,\n",
    "    'treated': treated\n",
    "})\n",
    "\n",
    "# Fit regression\n",
    "X_model = sm.add_constant(df[['X', 'treated']], has_constant='add')\n",
    "y_model = df['y']\n",
    "model = sm.OLS(y_model, X_model).fit()\n",
    "\n",
    "# Compute counterfactual\n",
    "df_untreated = df[df['treated'] == 0].copy()\n",
    "df_untreated['treated'] = 1 \n",
    "\n",
    "X_cf = sm.add_constant(df_untreated[['X', 'treated']], has_constant='add')\n",
    "X_cf = X_cf[model.params.index]  \n",
    "\n",
    "# Predict counterfactual\n",
    "df_untreated['pred_if_treated'] = model.predict(X_cf)\n",
    "\n",
    "# Get treatment effects and 90th percentile\n",
    "df_untreated['treatment_effect'] = (\n",
    "    df_untreated['pred_if_treated'] - df[df['treated'] == 0]['y'].values\n",
    ")\n",
    "\n",
    "mte_90th = np.percentile(df_untreated['treatment_effect'], 90)\n",
    "print(f\"Estimated Marginal Treatment Effect (90th percentile): {mte_90th:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework Reflection 7\n",
    "\n",
    "1. Create a linear regression model involving a confounder that is left out of the model.  Show whether the true correlation between X and Y is overestimated, underestimated, or neither.  Explain in words why this is the case for the given coefficients you have chosen.\n",
    "\n",
    "2. Perform a linear regression analysis in which one of the coefficients is zero, e.g.\n",
    "\n",
    "W = [noise]\n",
    "\n",
    "X = [noise]\n",
    "\n",
    "Y = 2 * X + [noise]\n",
    "\n",
    "And compute the p-value of a coefficient - in this case, the coefficient of W.  \n",
    "(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)\n",
    "If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)\n",
    "Run the analysis 1000 times and report the best (smallest) p-value.  \n",
    "If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection 7 Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(const    0.006134\n",
       " X        2.989823\n",
       " Z        5.027912\n",
       " dtype: float64,\n",
       " const   -0.068472\n",
       " X        5.073939\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Synthetic data\n",
    "n = 1000\n",
    "Z = np.random.normal(0, 1, n)\n",
    "epsilon_X = np.random.normal(0, 1, n)\n",
    "epsilon_Y = np.random.normal(0, 1, n)\n",
    "\n",
    "X = 0.6 * Z + epsilon_X\n",
    "Y = 3 * X + 5 * Z + epsilon_Y\n",
    "\n",
    "df = pd.DataFrame({'X': X, 'Z': Z, 'Y': Y})\n",
    "\n",
    "X_full = sm.add_constant(df[['X', 'Z']])\n",
    "model_full = sm.OLS(df['Y'], X_full).fit()\n",
    "\n",
    "# Confounder left out of model\n",
    "X_omit = sm.add_constant(df[['X']])\n",
    "model_omit = sm.OLS(df['Y'], X_omit).fit()\n",
    "\n",
    "# Extract coefficients\n",
    "coef_full = model_full.params\n",
    "coef_omit = model_omit.params\n",
    "\n",
    "coef_full, coef_omit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the true X coefficient is approximately 3.0, but omitting the confounder gives an X coefficient of 5.0. From the X and Y equations, Z has a positive correlation with both. When Z is omitted, the previous positive effect of Z on Y is then falsely attributed to X. This results in an overestimated X coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0004745704124689169)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "p_values = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    n = 100\n",
    "    W = np.random.normal(0, 1, n)\n",
    "    X = np.random.normal(0, 1, n)\n",
    "    noise = np.random.normal(0, 1, n)\n",
    "    Y = 2 * X + noise  \n",
    "\n",
    "    # Regress Y on both W and X\n",
    "    df = sm.add_constant(np.column_stack((W, X)))\n",
    "    model = sm.OLS(Y, df).fit()\n",
    "    \n",
    "    # Store the p-value for W \n",
    "    p_values.append(model.pvalues[1])\n",
    "\n",
    "# Minimum p-value\n",
    "min_p_value = np.min(p_values)\n",
    "min_p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the observed minimum p-value through the 1000 iterations is less than 0.05, the initial thought would be that the coefficient is nonzero. However, since the regression is setup so that W is 0, we know this is not true and that the true W coefficient is zero. The problem with repeating the analysis 1000 times is that when running many hypothesis tests, there are still some instances where the p-value will be small just by chance. Selecting the smallest p-value out of these iterations increases the chance of finding one of these instances and false rejecting the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework Reflection 8\n",
    "\n",
    "Include the code you used to solve the two coding quiz problems and write about the obstacles / challenges / insights you encountered while solving them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the insights gathered included that using Mahalanobis distance instead of Euclidean distance matched the items based on multivariate similarity. In this case, it was able to account for the scale of correlations of Z1 and Z2, giving more accurate counterfactuals. Another insight was that propensity score estimation is quite sensitive, even with just a single Z producing a wide range of scores through logistic regression. A challenge was refreshing myself on matrix manipulation in Python, used in the latter half of the problems. Functions like np.cov and the np.linalg library were helpful to revisit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection 8 Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.109218</td>\n",
       "      <td>1.764052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.259504</td>\n",
       "      <td>0.400157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.647584</td>\n",
       "      <td>0.978738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.106071</td>\n",
       "      <td>2.240893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.583464</td>\n",
       "      <td>1.867558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  X         Y         Z\n",
       "0           0  1  4.109218  1.764052\n",
       "1           1  0  2.259504  0.400157\n",
       "2           2  0 -0.647584  0.978738\n",
       "3           3  0  2.106071  2.240893\n",
       "4           4  1  3.583464  1.867558"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('homework_8.1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.2743411898510133)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_covariate = df[['Z']]\n",
    "treatment = df['X']\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_covariate, treatment)\n",
    "\n",
    "# Predict propensity scores\n",
    "propensity_scores = log_reg.predict_proba(X_covariate)[:, 1]\n",
    "\n",
    "# Inverse probability weights\n",
    "weights = np.where(df['X'] == 1, 1 / propensity_scores, 1 / (1 - propensity_scores))\n",
    "\n",
    "# Calculate weighted means\n",
    "weighted_y_treated = np.sum((df['Y'] * weights)[df['X'] == 1]) / np.sum(weights[df['X'] == 1])\n",
    "weighted_y_untreated = np.sum((df['Y'] * weights)[df['X'] == 0]) / np.sum(weights[df['X'] == 0])\n",
    "\n",
    "# ATE\n",
    "ate_ipw = weighted_y_treated - weighted_y_untreated\n",
    "ate_ipw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84011371, 0.58464597, 0.71108245])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first three propensity scores\n",
    "propensity_scores[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.437678997912609)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('homework_8.2.csv')\n",
    "\n",
    "treated = df2[df2['X'] == 1].reset_index(drop=True)\n",
    "untreated = df2[df2['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Extract covariates\n",
    "Z_all = df2[['Z1', 'Z2']].values.T\n",
    "cov_matrix = np.cov(Z_all)\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "treated_vectors = treated[['Z1', 'Z2']].values\n",
    "untreated_vectors = untreated[['Z1', 'Z2']].values\n",
    "\n",
    "# Compute all Mahalanobis distances\n",
    "all_distances = cdist(treated_vectors, untreated_vectors, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# Index of the nearest untreated unit for each treated unit\n",
    "nearest_indices = np.argmin(all_distances, axis=1)\n",
    "\n",
    "# Use indices to get matched outcomes from untreated group\n",
    "matched_outcomes = untreated.iloc[nearest_indices]['Y'].values\n",
    "\n",
    "# Calculate ATE\n",
    "ate_mahalanobis = (treated['Y'].values - matched_outcomes).mean()\n",
    "ate_mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.6962240525635797, 0.5381554886023228]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treated_vectors = treated[['Z1', 'Z2']].values\n",
    "untreated_vectors = untreated[['Z1', 'Z2']].values\n",
    "\n",
    "# Compute Mahalanobis distances for all treated and untreated pairs\n",
    "all_distances = cdist(treated_vectors, untreated_vectors, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# Find the index of the treated item \n",
    "min_distances = all_distances.min(axis=1)\n",
    "worst_match_index_no_lambda = np.argmax(min_distances)\n",
    "\n",
    "# Get the Z values\n",
    "nearest_z_values = treated.iloc[worst_match_index_no_lambda][['Z1', 'Z2']]\n",
    "nearest_z_values.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
